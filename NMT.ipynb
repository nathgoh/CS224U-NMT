{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acaffe52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "assert device == \"cuda\"\n",
    "\n",
    "# Keep random number generator consistent\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6d4ad9",
   "metadata": {},
   "source": [
    "## Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017aafbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_sentences, get_vocabs, get_max_len_sentences\n",
    "\n",
    "vocab_en = get_vocabs(\"vocab_vi.en\")\n",
    "vocab_de = get_vocabs(\"vocab.vi\")\n",
    "\n",
    "# Train sentences\n",
    "train_sentences_en = get_sentences(\"train_vi.en\")\n",
    "train_sentences_de = get_sentences(\"train.vi\")\n",
    "\n",
    "# Actual test sentences\n",
    "test_sentences_en = get_sentences(\"tst2013.en\")\n",
    "test_sentences_de = get_sentences(\"tst2013.vi\")\n",
    "\n",
    "# Filter sentences over n words long, in this case 48\n",
    "# The sentences will be n + 2 (50) words long when we include the <s>, </s> tokens\n",
    "MAX_LEN = 48\n",
    "MAX_LEN_WITH_TOKENS = 50\n",
    "\n",
    "train_sentences_en, train_sentences_de = get_max_len_sentences(train_sentences_en, train_sentences_de, MAX_LEN)\n",
    "test_sentences_en, test_sentences_de = get_max_len_sentences(test_sentences_en, test_sentences_de, MAX_LEN)\n",
    "\n",
    "# Make validation sets\n",
    "val_sentences_en = train_sentences_en[:int(len(train_sentences_en) * 0.1)]\n",
    "val_sentences_de = train_sentences_de[:int(len(train_sentences_de) * 0.1)]\n",
    "\n",
    "# Update training sets\n",
    "train_sentences_en = train_sentences_en[int(len(train_sentences_en) * 0.1):]\n",
    "train_sentences_de = train_sentences_de[int(len(train_sentences_en) * 0.1):]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818610dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_sentences_en), len(train_sentences_de)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2b006b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils import data\n",
    "\n",
    "PAD_INDEX = 0\n",
    "UNK_INDEX = 1\n",
    "SOS_INDEX = 2\n",
    "EOS_INDEX = 3\n",
    "\n",
    "class NMTDataset(data.Dataset):\n",
    "    def __init__(self, source_sentences, source_vocabs, target_sentences, target_vocabs):\n",
    "        self.max_sentence_length = MAX_LEN_WITH_TOKENS\n",
    "\n",
    "        self.source_sentences = source_sentences[:int(len(source_sentences))]\n",
    "        self.target_sentences = target_sentences[:int(len(source_sentences))]\n",
    "\n",
    "        self.source_vocabs = source_vocabs\n",
    "        self.target_vocabs = target_vocabs\n",
    "\n",
    "        self.source_vocab_ids = {v : i for i, v in enumerate(source_vocabs)}\n",
    "        self.source_id_to_vocabs = {val : key for key, val in self.source_vocab_ids.items()}\n",
    "        self.target_vocab_ids = {v : i for i, v in enumerate(target_vocabs)}\n",
    "        self.target_id_to_vocabs = {val : key for key, val in self.target_vocab_ids.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.source_sentences)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        source_sentence = self.source_sentences[index]\n",
    "    \n",
    "        # Add <s> and </s> to each source sentence\n",
    "        source_len = len(source_sentence) + 2   \n",
    "        source_id = []\n",
    "        for w in source_sentence:\n",
    "            if w not in self.source_vocabs:\n",
    "                w = '<unk>'\n",
    "            source_id.append(self.source_vocab_ids[w])\n",
    "\n",
    "        source_id = ([SOS_INDEX] + source_id + [EOS_INDEX] + [PAD_INDEX] * (self.max_sentence_length - source_len))\n",
    "        target_sentence = self.target_sentences[index]\n",
    "\n",
    "        # Add <s> and </s> to each target sentence\n",
    "        target_len = len(target_sentence) + 2\n",
    "        target_id = []\n",
    "        for w in target_sentence:\n",
    "            if w not in self.target_vocabs:\n",
    "                w = '<unk>'\n",
    "            target_id.append(self.target_vocab_ids[w])\n",
    "\n",
    "        target_id = ([SOS_INDEX] + target_id + [EOS_INDEX] + [PAD_INDEX] * (self.max_sentence_length - target_len))\n",
    "\n",
    "        return torch.tensor(source_id), source_len, torch.tensor(target_id), target_len"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacc4c5c",
   "metadata": {},
   "source": [
    "## Baseline Encoder Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee9c45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, dropout = 0):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = nn.LSTM(input_size, hidden_size, batch_first = True, dropout = dropout)\n",
    "    \n",
    "    def forward(self, inputs, lengths):       \n",
    "        packed_inputs = pack_padded_sequence(inputs, lengths.detach().cpu(), batch_first=True, enforce_sorted=False)\n",
    "        packed_outputs, hidden = self.rnn(packed_inputs)\n",
    "        outputs, output_lengths = pad_packed_sequence(packed_outputs)\n",
    "        \n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc962ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, dropout = 0):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = nn.LSTM(input_size, hidden_size, batch_first = True, dropout = dropout)\n",
    "        \n",
    "        # Layer of how we connect the final encoder state as the start for the decoder\n",
    "        self.encoder_to_decoder_layer = nn.Linear(hidden_size, hidden_size)\n",
    "        \n",
    "        self.dropout_layer = nn.Dropout(dropout)\n",
    "        self.input_hidden_to_hidden_layer = nn.Linear(input_size + hidden_size, hidden_size, bias = False)\n",
    "        \n",
    "    def forward_step(self, prev_embed, hidden):\n",
    "        # Does a single decoder step (one word)\n",
    "        rnn_out, hidden_out = self.rnn(prev_embed, hidden)\n",
    "        \n",
    "        output = torch.cat([prev_embed, rnn_out], dim = 2)\n",
    "        output = self.dropout_layer(output)\n",
    "        output = self.input_hidden_to_hidden_layer(output)\n",
    "        \n",
    "        return rnn_out, hidden_out, output\n",
    "        \n",
    "    def forward(self, inputs, final_encoder_states, hidden = None, max_len = None):\n",
    "        # Unroll the decoder one step at a time\n",
    "        if max_len is None:\n",
    "            max_len = inputs.size(1)\n",
    "        if hidden is None:\n",
    "            hidden = self.init_hidden(final_encoder_states)\n",
    "            \n",
    "        # Unrolling for decoder RNN for max_len steps\n",
    "        decoder_states = []\n",
    "        outputs = []\n",
    "        for j in range(max_len):\n",
    "            prev_embed = inputs[:, j].unsqueeze(1)\n",
    "            rnn_out, hidden_out, output = self.forward_step(prev_embed, hidden)\n",
    "            decoder_states.append(rnn_out)\n",
    "            outputs.append(output)\n",
    "        \n",
    "        decoder_states = torch.cat(decoder_states, dim = 1)\n",
    "        outputs = torch.cat(outputs, dim = 1)\n",
    "        \n",
    "        return hidden, outputs\n",
    "            \n",
    "    def init_hidden(self, final_encoder_states):\n",
    "        # Initialize first decoder hidden state using the final encoder hidden states\n",
    "        return torch.tanh(self.encoder_to_decoder_layer(final_encoder_states))\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47ad3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder, source_embed, target_embed, generator):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.source_embed = source_embed\n",
    "        self.target_embed = target_embed\n",
    "        self.generator = generator\n",
    "        \n",
    "    def forward(self, source_ids, target_ids, source_lengths):\n",
    "        encoder_hiddens, encoder_finals = self.encode(source_ids, source_lengths)\n",
    "        return self.decode(encoder_finals, target_ids[:, :-1])\n",
    "\n",
    "    def encode(self, source_ids, source_lengths):\n",
    "        return self.encoder(self.source_embed(source_ids), source_lengths)\n",
    "\n",
    "    def decode(self, final_encoder_states, target_ids, decoder_hidden=None):\n",
    "        return self.decoder(self.target_embed(target_ids), final_encoder_states, decoder_hidden) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748ebd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    # Define standard linear and softmax generation step\n",
    "    def __init__(self, hidden_size, vocab_size):\n",
    "        super(Generator, self).__init__()\n",
    "        self.proj = nn.Linear(hidden_size, vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.log_softmax(self.proj(x), dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2865b8bc",
   "metadata": {},
   "source": [
    "## Training \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285fdae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class LossCompute:\n",
    "    def __init__(self, generator, criterion, opt = None):\n",
    "        self.generator = generator\n",
    "        self.criterion = criterion\n",
    "        self.opt = opt\n",
    "\n",
    "    def __call__(self, x, y, norm):\n",
    "        x = self.generator(x)\n",
    "        loss = self.criterion(x.contiguous().view(-1, x.size(-1)), y.contiguous().view(-1))\n",
    "        loss = loss / norm\n",
    "\n",
    "        # Training mode\n",
    "        if self.opt is not None:  \n",
    "            loss.backward()          \n",
    "            self.opt.step()\n",
    "            self.opt.zero_grad()\n",
    "\n",
    "        return loss.data.item() * norm\n",
    "\n",
    "def run_epoch(data_loader, model, loss_compute, print_every):\n",
    "    total_tokens = 0\n",
    "    total_loss = 0\n",
    "\n",
    "    for j, (source_ids_batch, source_lengths_batch, target_ids_batch, target_lengths_batch) in enumerate(data_loader):\n",
    "        source_ids_batch = source_ids_batch.to(device)\n",
    "        source_lengths_batch = source_lengths_batch.to(device)\n",
    "        target_ids_batch = target_ids_batch.to(device)\n",
    "        \n",
    "        _, output = model(source_ids_batch, target_ids_batch, source_lengths_batch)\n",
    "\n",
    "        loss = loss_compute(x = output, y = target_ids_batch[:, 1:], norm = source_ids_batch.size(0))\n",
    "        total_loss += loss\n",
    "        total_tokens += (target_ids_batch[:, 1:] != PAD_INDEX).data.sum().item()\n",
    "\n",
    "        if model.training and j % print_every == 0:\n",
    "              print(\"Epoch Step: {} Loss: {}\".format(j, loss / source_ids_batch.size(0)))\n",
    "\n",
    "    return math.exp(total_loss / float(total_tokens))\n",
    "\n",
    "def train(model, num_epochs, learning_rate, print_every):\n",
    "    # ignore_index as PAD_INDEX so that pad tokens won't be included when computing the loss\n",
    "    criterion = nn.NLLLoss(reduction = \"sum\", ignore_index = PAD_INDEX)\n",
    "    optim = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "\n",
    "    # Keep track of dev perplexity for each epoch.\n",
    "    dev_perplexities = []\n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"Epoch\", epoch)\n",
    "\n",
    "        model.train()\n",
    "        train_perplexity = run_epoch(data_loader = train_data_loader, model = model, loss_compute = LossCompute(model.generator, criterion, optim), print_every = print_every)\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():      \n",
    "            dev_perplexity = run_epoch(data_loader = val_data_loader, model = model, loss_compute = LossCompute(model.generator, criterion, None), print_every = print_every)\n",
    "            print(\"Validation perplexity: {}\".format(dev_perplexity))\n",
    "            dev_perplexities.append(dev_perplexity)\n",
    "        \n",
    "    return dev_perplexities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c75482",
   "metadata": {},
   "source": [
    "## Baseline Encoder Decoder Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d512046e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "# English to German\n",
    "train_set = NMTDataset(train_sentences_en, vocab_en, train_sentences_de, vocab_de)\n",
    "train_data_loader = data.DataLoader(train_set, batch_size = batch_size, num_workers = 2, shuffle=True)\n",
    "\n",
    "val_set = NMTDataset(val_sentences_en, vocab_en, val_sentences_de, vocab_de)\n",
    "val_data_loader = data.DataLoader(val_set, batch_size = batch_size, num_workers = 2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6edfe13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters \n",
    "embed_size = 256   # Each word will be represented as a `embed_size`-dim vector.\n",
    "hidden_size = 512  # RNN hidden size\n",
    "dropout = 0.2\n",
    "\n",
    "baseline_seq2seq = EncoderDecoder(\n",
    "    encoder = Encoder(embed_size, hidden_size, dropout = dropout),\n",
    "    decoder = Decoder(embed_size, hidden_size, dropout = dropout),\n",
    "    source_embed = nn.Embedding(len(vocab_en), embed_size),\n",
    "    target_embed = nn.Embedding(len(vocab_de), embed_size),\n",
    "    generator = Generator(hidden_size, len(vocab_de))).to(device)\n",
    "\n",
    "train_model = True\n",
    "if train_model:\n",
    "    # Training, returns dev_perplexities, a list of dev perplexity for each epoch\n",
    "    pure_dev_perplexities = train(baseline_seq2seq, num_epochs = 10, learning_rate = 1e-3, print_every = 100)\n",
    "    torch.save(pure_seq2seq.state_dict(), \"baseline_seq2seq.pt\")\n",
    "\n",
    "    # Plot perplexity\n",
    "    utils.plot_perplexity(pure_dev_perplexities)\n",
    "else:\n",
    "    baseline_seq2seq.load_state_dict(torch.load(\"baseline_seq2seq.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_seq2seq"
   ]
  },
  {
   "source": [
    "## Baseline Decoding"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(model, source_ids, source_lengths, max_len):\n",
    "    with torch.no_grad():\n",
    "        _, final_encoder_states = model.encode(source_ids, source_lengths)\n",
    "        prev_y = torch.ones(1, 1).fill_(SOS_INDEX).type_as(source_ids)\n",
    "\n",
    "    for i in range(max_len):\n",
    "        with torch.no_grad():\n",
    "            hidden, outputs = model.decode(final_encoder_states, prev_y, hidden)\n",
    "            prob = model.generator(outputs[:, -1])\n",
    "            d, next_word = torch.max(prob, dim = 1)\n",
    "            next_word = next_word.data.item()\n",
    "            output.append(next_word)\n",
    "            prev_y = torch.ones(1, 1).type_as(source_ids).fill_(next_word)\n",
    "    output = np.array(output)\n",
    "\n",
    "    EOS_mask = np.where(output == EOS_INDEX)\n",
    "    if len(eos_mask[0]) > 0:\n",
    "        output = output[:EOS_mask[0][0]]\n",
    "\n",
    "    return output"
   ]
  },
  {
   "source": [
    "## BLEU Testing\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sacrebleu\n",
    "\n",
    "def compute_BLEU(model, data_loader, decoder, target_vocabs, attention = False)\n",
    "  bleu_score = []\n",
    "\n",
    "  model.eval()\n",
    "  for source_ids, source_lengths, target_ids, _ in data_loader:\n",
    "    if not with_attention:\n",
    "      result = decoder(model, source_ids.to(device), source_lengths.to(device), max_len = MAX_LEN_WITH_TOKENS)\n",
    "    else:\n",
    "      result, _ = decoder(model, source_ids.to(device), source_lengths.to(device), max_len = MAX_LEN_WITH_TOKENS)\n",
    "\n",
    "    # Remove <s>\n",
    "    source_ids = source_ids[0, 1:]\n",
    "    target_ids = target_ids[0, 1:]\n",
    "\n",
    "    # Remove </s> and <pad>\n",
    "    source_ids = source_ids[:np.where(source_ids == EOS_INDEX)[0][0]]\n",
    "    target_ids = target_ids[:np.where(target_ids == EOS_INDEX)[0][0]]\n",
    "\n",
    "    prediction = \" \".join(utils.lookup_words(result, vocab=target_vocabs))\n",
    "    target = \" \".join(utils.lookup_words(target_ids, vocab=target_vocabs))\n",
    "\n",
    "    bleu_score.append(sacrebleu.raw_corpus_bleu([pred], [[targ]], .01).score)\n",
    "\n",
    "  return bleu_score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python385jvsc74a57bd033a1378092e5cd1eb85e90719efd5bd5246634928c384bfd2d4c8ebeefa4de1b",
   "display_name": "Python 3.8.5 64-bit ('nlu': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}